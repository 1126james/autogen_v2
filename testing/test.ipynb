{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the project root directory (assuming your notebook is in a subdirectory)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Python lib\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any, Tuple\n",
    "import venv\n",
    "\n",
    "# Autogen-0.4\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.agents._code_executor_agent import CodeExecutorAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# Local\n",
    "from utils import CopyFile, LoadDataset, GetDatasetProfile, Spinner\n",
    "from prompts import cleaning_reasoning_prompt, cleaning_checking_prompt\n",
    "\n",
    "\n",
    "# Only edit here AND filepath under if __name__ == \"__main__\":\n",
    "reasoning_model = \"qwen2.5:32b-instruct-q8_0\"\n",
    "coding_model = \"qwen2.5-coder:32b-instruct-q8_0\"\n",
    "\n",
    "# Common config\n",
    "llm_base_url = \"http://34.204.63.234:11434/v1\"\n",
    "api_key = \"none\"\n",
    "capabilities =  {\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": False,\n",
    "        \"json_output\": False\n",
    "    }\n",
    "\n",
    "#######################################################################\n",
    "#   !!! DONT EDIT BELOW EXCEPT FOR if __name__ == \"__main__\":   !!!   #\n",
    "#######################################################################\n",
    "\n",
    "# Reasoning Model Configuration\n",
    "instruct_client_config = OpenAIChatCompletionClient(\n",
    "    model=reasoning_model,\n",
    "    base_url=llm_base_url,\n",
    "    api_key=api_key,\n",
    "    model_capabilities=capabilities\n",
    ")\n",
    "\n",
    "# Coding Model Configuration\n",
    "code_client_config = OpenAIChatCompletionClient(\n",
    "    model=coding_model,\n",
    "    base_url=llm_base_url,\n",
    "    api_key=api_key,\n",
    "    model_capabilities=capabilities\n",
    ")\n",
    "\n",
    "async def create_cleaning_agents(filepath: Path) -> Tuple[AssistantAgent, AssistantAgent]:\n",
    "    \n",
    "    # tqdm progress bar\n",
    "    with tqdm(total=3,\n",
    "             desc=\"Creating cleaning reasoning team agents\",\n",
    "             bar_format='{desc:>30}{postfix: <1} {bar}|{n_fmt:>4}/{total_fmt:<4}',\n",
    "             colour='green') as pbar:\n",
    "        \n",
    "        ### 1.1 Cleaning reasoning agent - 1\n",
    "        async def create_cleaning_reasoning_1():\n",
    "            Data_Cleaning_Planner = AssistantAgent(\n",
    "                name=\"Data_Cleaning_Planner\",\n",
    "                model_client=instruct_client_config,\n",
    "                system_message=cleaning_reasoning_prompt(filepath),\n",
    "            )\n",
    "            pbar.update(1)\n",
    "            return Data_Cleaning_Planner\n",
    "        \n",
    "        async def create_cleaning_reasoning_2():\n",
    "            Validation_Assistant = AssistantAgent(\n",
    "                name=\"Validation_Assistant\",\n",
    "                model_client=instruct_client_config,\n",
    "                system_message=cleaning_checking_prompt(filepath),\n",
    "            )\n",
    "            pbar.update(1)\n",
    "            return Validation_Assistant\n",
    "        \n",
    "        list_of_cleaning_reasoning_agents = await asyncio.gather(\n",
    "            create_cleaning_reasoning_1(),\n",
    "            create_cleaning_reasoning_2(),\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        return list_of_cleaning_reasoning_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cleaning_reasoning_pipeline(data_dict: Dict[str, Any], filepath: Path):\n",
    "    \"\"\"\n",
    "    Run the complete data cleaning and transformation pipeline\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # cleaning_reasoning_agent, \n",
    "        cleaning_reasoning_team = await create_cleaning_agents(filepath)\n",
    "        # cleaning_team = [cleaning_team[0], cleaning_team[1]]\n",
    "        \n",
    "        # Setup termination conditions\n",
    "        status_pass = TextMentionTermination(\"Overall Status: Pass\")\n",
    "        max_round = MaxMessageTermination(5)\n",
    "        termination = status_pass | max_round\n",
    "\n",
    "        # First phase: Data Cleaning (Reasoning)\n",
    "        cleaning_team_chat = RoundRobinGroupChat(\n",
    "            cleaning_reasoning_team,\n",
    "            termination_condition=termination,\n",
    "        )\n",
    "        \n",
    "        # Save last n messages\n",
    "        chat_history = BufferedChatCompletionContext(buffer_size=1)\n",
    "        \n",
    "        # A loading spinner to know if the code is frozen or not\n",
    "        #run_chat =  Spinner.async_with_spinner(\n",
    "        #    message=\"Loading: \",\n",
    "        #    style=\"braille\",\n",
    "        #    console_class=Console,\n",
    "        #    coroutine=cleaning_team_chat.run_stream(task=cleaning_reasoning_prompt(filepath, data_dict), cancellation_token=None)\n",
    "        #)\n",
    "        \n",
    "        # Uncomment below to run the code without spinner\n",
    "        stream = cleaning_team_chat.run_stream(task=cleaning_reasoning_prompt(filepath, data_dict), cancellation_token=None)\n",
    "        # await context.add_message(await Console(stream))\n",
    "        await chat_history.add_message(await Console(stream))\n",
    "        return chat_history\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         Processing 24 columns  \u001b[32m██████████\u001b[0m|  24/24  \n",
      "             Preparing dataset  \u001b[32m██████████\u001b[0m|   3/3   \n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=3,\n",
    "              desc=\"Preparing dataset\",\n",
    "              bar_format='{desc:>30}{postfix: <1} {bar}|{n_fmt:>4}/{total_fmt:<4}',\n",
    "              colour='green') as pbar:\n",
    "        # Edit file path\n",
    "        test_file = Path(\"/Users/jamesbond/Desktop/VSCode/AutoInsight/autogen_v2/sheets/credit_card_transactions.csv\")\n",
    "        pbar.update(1)\n",
    "        df = LoadDataset(test_file)\n",
    "        pbar.update(1)\n",
    "        # Custom function in utils/ to get data dict in markdown/natural language/json format\n",
    "        initial_profile = GetDatasetProfile(df, output_format=\"json\")\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating cleaning reasoning team agents  \u001b[32m██████████\u001b[0m|   3/3   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "<Data_Cleaning_Planner>\n",
      "\n",
      "<purpose>\n",
      "    Generate comprehensive and actionable data cleaning recommendations tailored to each column, incorporating domain-specific insights.\n",
      "</purpose>\n",
      "\n",
      "<dataset_location>\n",
      "    Filepath: /Users/jamesbond/Desktop/VSCode/AutoInsight/autogen_v2/sheets/credit_card_transactions.csv\n",
      "</dataset_location>\n",
      "\n",
      "<data_dictionary>\n",
      "    {'Unnamed: 0': {'dtype': 'int64', 'sample': 0, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 1296675, 'mean': np.float64(648337.0), 'median': np.float64(648337.0), 'std': np.float64(374317.97), 'min': np.int64(0), 'max': np.int64(1296674)}, 'trans_date_trans_time': {'dtype': 'object', 'sample': '2019-01-01 00:00:18', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 1274791}, 'cc_num': {'dtype': 'int64', 'sample': 2703186189652095, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 983, 'mean': np.float64(4.1719204207972666e+17), 'median': np.float64(3521417320836166.0), 'std': np.float64(1.3088064470002404e+18), 'min': np.int64(60416207185), 'max': np.int64(4992346398065154184)}, 'merchant': {'dtype': 'object', 'sample': 'fraud_Rippin, Kub and Mann', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 693}, 'category': {'dtype': 'object', 'sample': 'misc_net', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 14}, 'amt': {'dtype': 'float64', 'sample': 4.97, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 52928, 'mean': np.float64(70.35), 'median': np.float64(47.52), 'std': np.float64(160.32), 'min': np.float64(1.0), 'max': np.float64(28948.9)}, 'first': {'dtype': 'object', 'sample': 'Jennifer', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 352}, 'last': {'dtype': 'object', 'sample': 'Banks', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 481}, 'gender': {'dtype': 'object', 'sample': 'F', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 2}, 'street': {'dtype': 'object', 'sample': '561 Perry Cove', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 983}, 'city': {'dtype': 'object', 'sample': 'Moravian Falls', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 894}, 'state': {'dtype': 'object', 'sample': 'NC', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 51}, 'zip': {'dtype': 'int64', 'sample': 28654, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 970, 'mean': np.float64(48800.67), 'median': np.float64(48174.0), 'std': np.float64(26893.22), 'min': np.int64(1257), 'max': np.int64(99783)}, 'lat': {'dtype': 'float64', 'sample': 36.08, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 968, 'mean': np.float64(38.54), 'median': np.float64(39.35), 'std': np.float64(5.08), 'min': np.float64(20.03), 'max': np.float64(66.69)}, 'long': {'dtype': 'float64', 'sample': -81.18, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 969, 'mean': np.float64(-90.23), 'median': np.float64(-87.48), 'std': np.float64(13.76), 'min': np.float64(-165.67), 'max': np.float64(-67.95)}, 'city_pop': {'dtype': 'int64', 'sample': 3495, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 879, 'mean': np.float64(88824.44), 'median': np.float64(2456.0), 'std': np.float64(301956.36), 'min': np.int64(23), 'max': np.int64(2906700)}, 'job': {'dtype': 'object', 'sample': 'Psychologist, counselling', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 494}, 'dob': {'dtype': 'object', 'sample': '1988-03-09', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 968}, 'trans_num': {'dtype': 'object', 'sample': '0b242abb623afc578575680df30655b9', 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 1296675}, 'unix_time': {'dtype': 'int64', 'sample': 1325376018, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 1274823, 'mean': np.float64(1349243636.73), 'median': np.float64(1349249747.0), 'std': np.float64(12841278.42), 'min': np.int64(1325376018), 'max': np.int64(1371816817)}, 'merch_lat': {'dtype': 'float64', 'sample': 36.01, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 1247805, 'mean': np.float64(38.54), 'median': np.float64(39.37), 'std': np.float64(5.11), 'min': np.float64(19.03), 'max': np.float64(67.51)}, 'merch_long': {'dtype': 'float64', 'sample': -82.05, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 1275745, 'mean': np.float64(-90.23), 'median': np.float64(-87.44), 'std': np.float64(13.77), 'min': np.float64(-166.67), 'max': np.float64(-66.95)}, 'is_fraud': {'dtype': 'int64', 'sample': 0, 'total_count': 1296675, 'null_count': 0, 'null_percentage': '0.0%', 'unique_count': 2, 'mean': np.float64(0.01), 'median': np.float64(0.0), 'std': np.float64(0.08), 'min': np.int64(0), 'max': np.int64(1)}, 'merch_zipcode': {'dtype': 'float64', 'sample': 28705.0, 'total_count': 1296675, 'null_count': 195973, 'null_percentage': '15.1%', 'unique_count': 28336, 'mean': np.float64(46825.75), 'median': np.float64(45860.0), 'std': np.float64(25834.0), 'min': np.float64(1001.0), 'max': np.float64(99403.0)}}\n",
      "</data_dictionary>\n",
      "\n",
      "<instructions>\n",
      "    1. **Thoroughly examine** the provided data dictionary.\n",
      "    2. **Assess** data types and their real-world implications.\n",
      "    3. **Identify** potential data quality issues without restricting to predefined categories by considering:\n",
      "       - **Inter-column relationships** (e.g., geographic data, dates)\n",
      "       - **Data type limitations**\n",
      "       - **Domain-specific valid ranges**\n",
      "       - **Business logic dependencies**\n",
      "       - **Any other anomalies or irregularities** present in the data\n",
      "    4. **For each identified issue**, provide **one distinct data cleaning technique**:\n",
      "       - **Do NOT offer multiple alternatives or propose evaluations.**\n",
      "       - **Be specific** in your recommendations (e.g., \"Remove outliers that exceed three standard deviations from the mean\").\n",
      "       - Ensure the technique is **directly implementable** using standard Python libraries such as Pandas, NumPy, or Scikit-learn.\n",
      "    5. **Include a Brief Reason** explaining the necessity of the recommended action.\n",
      "    6. **Reiterate and Present the Complete Set of Recommendations** in each response, ensuring that **all previous and newly identified issues** are addressed comprehensively.\n",
      "    7. Ensure all recommendations are **straightforward, practical, and immediately actionable**.\n",
      "</instructions>\n",
      "\n",
      "<output_format>\n",
      "    For each column with identified issues, provide the following information:\n",
      "    1. **Column Name:** Exact name of the column as specified in the data dictionary.\n",
      "    2. **Issue Type:** Describe the nature of the data quality issue identified (e.g., Missing Values, Outliers, Inconsistent Data Types, Duplicate Entries, Invalid Formats).\n",
      "    3. **Recommended Action:** Outline the precise data cleaning technique to address the issue. Ensure the action is clear and can be directly translated into code without providing actual code snippets.\n",
      "    4. **Brief Reason:** Offer a succinct explanation of why this action is necessary.\n",
      "    5. **Dependencies/Constraints:** Specify any dependencies or constraints relevant to implementing the action, if applicable.\n",
      "</output_format>\n",
      "\n",
      "<output_example>\n",
      "### Column_Name\n",
      "- **Issue Type:** Missing Values\n",
      "- **Recommended Action:** Impute missing values with the median of the column.\n",
      "- **Brief Reason:** Imputing with the median preserves the central tendency without being influenced by outliers.\n",
      "- **Dependencies/Constraints:** Ensure that median imputation aligns with the data distribution and does not distort related features.\n",
      "\n",
      "---\n",
      "</output_example>\n",
      "\n",
      "<strict_rules>\n",
      "    1. **Use EXACT column names** as provided in the data dictionary.\n",
      "    2. Recommend only **BASIC data cleaning actions** that are easy to implement.\n",
      "    3. Provide **CLEAR and SIMPLE explanations**, avoiding technical jargon.\n",
      "    4. Emphasize **PRACTICAL SOLUTIONS** that can be directly translated into code.\n",
      "    5. Ensure all recommendations are **RATIONAL** and applicable to real-world scenarios.\n",
      "    6. **DO NOT** pose any questions or request clarifications.\n",
      "    7. **OMIT** any Exploratory Data Analysis (EDA) steps and visualizations.\n",
      "    8. **Acknowledge and respect inter-column relationships** (e.g., handle latitude and longitude together).\n",
      "    9. **Strictly adhere** to data type constraints (e.g., appropriately handle categorical vs. continuous data).\n",
      "    10. **DO NOT** provide multiple options, use conditional phrases (e.g., \"if possible\"), or suggest further evaluations. **Select and specify one definitive action** for each issue.\n",
      "    11. **Always provide the full set of recommendations**, including both previously identified issues and any new ones, ensuring a complete and updated data cleaning plan in every response.\n",
      "    12. **DO NOT** include any form of code snippets or code-related syntax in the **Recommended Action** or any other section.\n",
      "</strict_rules>\n",
      "\n",
      "</Data_Cleaning_Planner>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Data_Cleaning_Planner ----------\n",
      "### Column_Name: merch_zipcode\n",
      "- **Issue Type:** Missing Values\n",
      "- **Recommended Action:** Impute missing values with the median zipcode of the column.\n",
      "- **Brief Reason:** Using the median helps maintain a central value for zipcodes and minimizes distortion caused by outliers or gaps in data.\n",
      "- **Dependencies/Constraints:** Ensure that median imputation aligns with regional distribution to avoid introducing significant bias.\n",
      "\n",
      "### Column_Name: lat, lon (considering inter-column relationship)\n",
      "- **Issue Type:** Outliers\n",
      "- **Recommended Action:** Remove or correct geographic outliers based on standard deviation from the mean latitude and longitude coordinates.\n",
      "- **Brief Reason:** This action helps ensure that the location data accurately reflects real-world positions by removing extreme values that could skew analysis results.\n",
      "- **Dependencies/Constraints:** Ensure removal of outliers does not significantly reduce sample size.\n",
      "\n",
      "### Column_Name: merch_zipcode\n",
      "- **Issue Type:** Potential Invalid Format\n",
      "- **Recommended Action:** Replace any non-standard zipcode formats (e.g., decimals) with a standard integer format and correct using the nearest valid zipcode based on geographic centroids.\n",
      "- **Brief Reason:** Inconsistent formatting can cause errors in geospatial analysis or categorization. Standardizing to integers and correcting ensures consistency and accuracy.\n",
      "\n",
      "### Column_Name: merch_zipcode\n",
      "- **Issue Type:** Potential Geographic Mismatch (implied by high missing rate)\n",
      "- **Recommended Action:** Cross-check zipcodes against a known geographic database to ensure correct mapping, potentially updating erroneous entries based on nearest valid location.\n",
      "- **Brief Reason:** Ensuring accurate zipcode mappings helps in aligning merchant locations correctly with their actual geopolitical zones, crucial for analysis and reporting accuracy.\n",
      "\n",
      "### Column_Name: merch_zipcode\n",
      "- **Issue Type:** High Missing Percentage (15.1%)\n",
      "- **Recommended Action:** For high missing percentages, consider removing rows where the missing data cannot be reasonably imputed based on available geographic context.\n",
      "- **Brief Reason:** Removing such entries prevents misleading analysis results by excluding unreliable data points.\n",
      "\n",
      "### Column_Name: merch_zipcode\n",
      "- **Issue Type:** Outliers in Zipcode Distribution (high/min values)\n",
      "- **Recommended Action:** Correct or remove zipcodes that fall outside a defined valid range based on common U.S. postal code structures.\n",
      "- **Brief Reason:** Ensuring geographic accuracy prevents skewed analysis results and maintains data integrity.\n",
      "\n",
      "### Column_Name: date-related fields (like 'date' in 'merch_zipcode' context)\n",
      "- **Issue Type:** Consistency across Date Fields\n",
      "- **Recommended Action:** Ensure all date fields are consistent and properly formatted as datetime objects, correcting any inconsistencies.\n",
      "- **Brief Reason:** Uniform date formats prevent analysis errors and ensure chronological consistency.\n",
      "\n",
      "### Column_Name: lat, lon (considering inter-column relationship)\n",
      "- **Issue Type:** Data Type Limitation\n",
      "- **Recommended Action:** Convert latitude and longitude columns to float data types if they are stored as strings, ensuring proper numeric operations.\n",
      "- **Brief Reason:** Maintaining consistent and compatible data types is essential for performing accurate numeric operations on geographic coordinates.\n",
      "\n",
      "### Column_Name: date-related fields (like 'date' in 'merch_zipcode' context)\n",
      "- **Issue Type:** Missing or Incorrect Date Entries\n",
      "- **Recommended Action:** Identify and correct missing or incorrectly formatted date entries using the median date within a specific time frame.\n",
      "- **Brief Reason:** Ensures uniformity and completeness of temporal data for accurate time-based analysis.\n",
      "\n",
      "### Column_Name: all numeric fields (like merch_zipcode, lat, lon)\n",
      "- **Issue Type:** Potential Data Entry Errors\n",
      "- **Recommended Action:** Implement a range check to remove or correct values that fall outside defined valid ranges based on domain knowledge.\n",
      "- **Brief Reason:** Prevents inclusion of unlikely or impossible data entries, ensuring the dataset’s integrity and reliability.\n",
      "\n",
      "### Column_Name: merch_zipcode\n",
      "- **Issue Type:** High Cardinality (distinct values)\n",
      "- **Recommended Action:** Group zipcodes into broader regions using a spatial database to reduce complexity and improve analysis efficiency.\n",
      "- **Brief Reason:** Reduces data complexity while maintaining meaningful geographic segments for effective analysis.\n",
      "\n",
      "Reiterate all the previously identified issues with their respective actions:\n",
      "\n",
      "### Column_Name: merch_zipcode\n",
      "- **Issue Type:** Missing Values & Invalid Format & Geographic Mismatch & High Cardinality\n",
      "- **Recommended Action(s):**\n",
      "  - **Missing Values:** Impute missing values with the median zipcode.\n",
      "  - **Invalid Format:** Replace non-standard formats with standard integer format.\n",
      "  - **Geographic Mismatch:** Cross-check zipcodes against a known geographic database.\n",
      "  - **High Cardinality:** Group into broader regions using a spatial database.\n",
      "- **Brief Reasons:**\n",
      "  - Imputing preserves central tendency.\n",
      "  - Standard formatting avoids errors in geospatial analysis.\n",
      "  - Correct mapping prevents misleading results.\n",
      "  - Reduces complexity and improves meaningful segments.\n",
      "\n",
      "### Column_Name: lat, lon\n",
      "- **Issue Type:** Outliers & Data Type Limitation & Inter-column Relationship\n",
      "- **Recommended Action(s):**\n",
      "  - **Outliers:** Remove or correct based on standard deviations from mean coordinates.\n",
      "  - **Data Type Conversion:** Ensure consistent float data types for numeric operations.\n",
      "  - **Inter-column Consistency:** Use both lat and lon consistently.\n",
      "- **Brief Reasons:**\n",
      "  - Removal of extreme values maintains accuracy.\n",
      "  - Consistent types ensure proper operations.\n",
      "  - Uniform use avoids skewed results.\n",
      "\n",
      "### Column_Name: date-related fields\n",
      "- **Issue Type:** Missing or Incorrect Date Entries & Consistency across Date Fields\n",
      "- **Recommended Action(s):**\n",
      "  - Correct inconsistencies using median date within a time frame.\n",
      "  - Ensure uniform formatting as datetime objects.\n",
      "- **Brief Reasons:**\n",
      "  - Uniform dates prevent analysis errors.\n",
      "  - Proper correction maintains dataset integrity.\n",
      "\n",
      "### Column_Name: all numeric fields (like lat, lon)\n",
      "- **Issue Type:** Potential Data Entry Errors\n",
      "- **Recommended Action(s):**\n",
      "  - Use range checks to remove or correct values outside valid ranges based on domain knowledge.\n",
      "- **Brief Reason:**\n",
      "  - Ensures removal of unrealistic data points. \n",
      "\n",
      "All actions collectively help ensure the overall integrity and accuracy of geographic and time-based datasets, enabling reliable analysis and reporting. \n",
      "\n",
      "By following these recommendations, we can significantly enhance data quality and reduce potential errors in geographic and temporal analyses. \n",
      "\n",
      "These actions are critical for ensuring that any subsequent analysis based on this dataset is both meaningful and accurate. The steps provided account for common issues encountered with such datasets and aim to address them systematically to achieve a robust and reliable dataset suitable for advanced analytical tasks. \n",
      "\n",
      "In summary, these measures will enhance the accuracy, integrity, and usability of the dataset, ensuring it remains valuable across various analytical scenarios and applications. \n",
      "\n",
      "By consistently applying these best practices, we can leverage high-quality data in meaningful ways that drive informed decision-making processes effectively. Enhance Data Integrity Through Comprehensive Checks\n",
      "\n",
      "This approach ensures that the dataset remains free from inaccuracies and inconsistencies, providing a solid foundation for subsequent analyses and business decisions.\n",
      "[Prompt tokens: 2048, Completion tokens: 1423]\n",
      "---------- Validation_Assistant ----------\n",
      "### Summary of Identified Issues and Recommended Actions\n",
      "\n",
      "#### Column_Name: **merch_zipcode**\n",
      "- **Issue Type:** Missing Values & Invalid Format & Geographic Mismatch & High Cardinality\n",
      "- **Recommended Action(s):**\n",
      "  - **Missing Values:** Impute missing values with the median zipcode.\n",
      "  - **Invalid Format:** Replace non-standard formats with standard integer format.\n",
      "  - **Geographic Mismatch:** Cross-check zipcodes against a known geographic database.\n",
      "  - **High Cardinality:** Group into broader regions using a spatial database.\n",
      "- **Brief Reasons:**\n",
      "  - Imputing preserves central tendency.\n",
      "  - Standard formatting avoids errors in geospatial analysis.\n",
      "  - Correct mapping prevents misleading results.\n",
      "  - Reduces complexity and improves meaningful segments.\n",
      "\n",
      "#### Column_Name: **lat, lon**\n",
      "- **Issue Type:** Outliers & Data Type Limitation & Inter-column Relationship\n",
      "- **Recommended Action(s):**\n",
      "  - **Outliers:** Remove or correct based on standard deviations from mean coordinates.\n",
      "  - **Data Type Conversion:** Ensure consistent float data types for numeric operations.\n",
      "  - **Inter-column Consistency:** Use both lat and lon consistently.\n",
      "- **Brief Reasons:**\n",
      "  - Removal of extreme values maintains accuracy.\n",
      "  - Consistent types ensure proper operations.\n",
      "  - Uniform use avoids skewed results.\n",
      "\n",
      "#### Column_Name: **date-related fields**\n",
      "- **Issue Type:** Missing or Incorrect Date Entries & Consistency across Date Fields\n",
      "- **Recommended Action(s):**\n",
      "  - Correct inconsistencies using median date within a time frame.\n",
      "  - Ensure uniform formatting as datetime objects.\n",
      "- **Brief Reasons:**\n",
      "  - Uniform dates prevent analysis errors.\n",
      "  - Proper correction maintains dataset integrity.\n",
      "\n",
      "#### Column_Name: **all numeric fields (like lat, lon)**\n",
      "- **Issue Type:** Potential Data Entry Errors\n",
      "- **Recommended Action(s):**\n",
      "  - Use range checks to remove or correct values outside valid ranges based on domain knowledge.\n",
      "- **Brief Reason:**\n",
      "  - Ensures removal of unrealistic data points.\n",
      "\n",
      "---\n",
      "\n",
      "### Detailed Recommendations\n",
      "\n",
      "#### Column_Name: **merch_zipcode**\n",
      "1. **Missing Values:**\n",
      "   - Impute missing values with the median zipcode value.\n",
      "   \n",
      "2. **Invalid Format:**\n",
      "   - Replace non-standard formats (e.g., strings, invalid numbers) with standard integer format.\n",
      "  \n",
      "3. **Geographic Mismatch:**\n",
      "   - Cross-check zipcodes against a known geographic dataset to ensure accuracy.\n",
      "\n",
      "4. **High Cardinality:**\n",
      "   - Group zipcodes into broader regions using a spatial database for better analysis and reduced complexity.\n",
      "\n",
      "#### Column_Name: **lat, lon**\n",
      "1. **Outliers:**\n",
      "   - Identify outliers based on standard deviations from the mean coordinates.\n",
      "   - Correct or remove these values to maintain accuracy.\n",
      "\n",
      "2. **Data Type Limitation:**\n",
      "   - Convert latitude and longitude columns to float data types if stored as strings, ensuring compatible numeric operations.\n",
      "\n",
      "3. **Inter-column Relationship:**\n",
      "   - Ensure consistent use of lat and lon pairs across records for uniformity in analysis.\n",
      "\n",
      "#### Column_Name: **date-related fields**\n",
      "1. **Missing or Incorrect Date Entries:**\n",
      "   - Identify missing or incorrectly formatted dates.\n",
      "   - Correct using the median date within a specific time frame.\n",
      "\n",
      "2. **Consistency across Date Fields:**\n",
      "   - Ensure that all date entries are consistent and properly formatted as datetime objects to prevent errors in analysis.\n",
      "\n",
      "#### Column_Name: **all numeric fields (like lat, lon)**\n",
      "1. **Potential Data Entry Errors:**\n",
      "   - Implement range checks to identify and correct values outside valid ranges based on domain knowledge.\n",
      "   \n",
      "### Summary\n",
      "\n",
      "To ensure the overall integrity and accuracy of the dataset:\n",
      "\n",
      "- Address missing or invalid data formats.\n",
      "- Correct geographic mismatches using known databases.\n",
      "- Reduce high cardinality by grouping into broader regions.\n",
      "- Remove outliers based on standard deviations for consistency.\n",
      "- Ensure all related date fields maintain uniform formatting as datetime objects.\n",
      "- Validate numeric values within defined valid ranges.\n",
      "\n",
      "These actions collectively help to ensure the quality and reliability of your dataset, making it suitable for various analytical tasks.\n",
      "[Prompt tokens: 2048, Completion tokens: 831]\n",
      "---------- Data_Cleaning_Planner ----------\n",
      "### Summary of Identified Issues and Recommended Actions\n",
      "\n",
      "#### Column_Name: **merch_zipcode**\n",
      "- **Issue Type:** Missing Values & Invalid Format & Geographic Mismatch & High Cardinality\n",
      "- **Recommended Action(s):**\n",
      "  - **Missing Values:** Impute missing values with the median zipcode.\n",
      "  - **Invalid Format:** Replace non-standard formats with standard integer format.\n",
      "  - **Geographic Mismatch:** Cross-check zipcodes against a known geographic database and correct any mismatches.\n",
      "  - **High Cardinality:** Group into broader regions using a spatial database.\n",
      "- **Brief Reasons:**\n",
      "  - Imputing preserves central tendency and avoids distortions in analysis.\n",
      "  - Standard formatting ensures consistency in geospatial analysis.\n",
      "  - Correct mapping prevents misleading or incorrect results.\n",
      "  - Reducing complexity improves meaningful segments for further analysis.\n",
      "\n",
      "#### Column_Name: **lat, lon**\n",
      "- **Issue Type:** Outliers & Data Type Limitation & Inter-column Relationship\n",
      "- **Recommended Action(s):**\n",
      "  - **Outliers:** Remove coordinates outside a valid range defined by standard deviations from the mean.\n",
      "  - **Data Type Conversion:** Ensure consistent float data types for numeric operations and analysis.\n",
      "  - **Inter-column Consistency:** Validate lat-long pairs to ensure they are used consistently across records.\n",
      "- **Brief Reasons:**\n",
      "  - Removal of extreme values maintains accuracy and reliability.\n",
      "  - Consistent types prevent errors in calculations or transformations.\n",
      "  - Uniform use avoids skewed results and ensures spatial consistency.\n",
      "\n",
      "#### Column_Name: **date-related fields**\n",
      "- **Issue Type:** Missing or Incorrect Date Entries & Consistency across Date Fields\n",
      "- **Recommended Action(s):**\n",
      "  - Correct inconsistencies using the median date within a specific time frame.\n",
      "  - Ensure uniform formatting as datetime objects for all date entries.\n",
      "- **Brief Reasons:**\n",
      "  - Uniform dates prevent errors in temporal analysis and ensure consistency.\n",
      "  - Proper correction maintains dataset integrity and reliability.\n",
      "\n",
      "#### Column_Name: **all numeric fields (like lat, lon)**\n",
      "- **Issue Type:** Potential Data Entry Errors\n",
      "- **Recommended Action(s):**\n",
      "  - Use range checks to remove or correct values outside valid ranges based on domain knowledge.\n",
      "- **Brief Reason:**\n",
      "  - Ensures removal of unrealistic data points and improves overall data quality.\n",
      "\n",
      "---\n",
      "\n",
      "### Detailed Recommendations\n",
      "\n",
      "#### Column_Name: **merch_zipcode**\n",
      "1. **Missing Values:**\n",
      "   - Impute missing values with the median zipcode value to maintain consistent geographic representation.\n",
      "  \n",
      "2. **Invalid Format:**\n",
      "   - Replace non-standard formats (e.g., strings, invalid numbers) with standard integer format for consistency and accuracy.\n",
      "\n",
      "3. **Geographic Mismatch:**\n",
      "   - Cross-check zipcodes against a known geographic dataset to ensure that each entry corresponds accurately to its respective region or location.\n",
      "\n",
      "4. **High Cardinality:**\n",
      "   - Group zipcodes into broader regions using a spatial database (e.g., city, state) to reduce complexity for better analysis and segmentation.\n",
      "\n",
      "#### Column_Name: **lat, lon**\n",
      "1. **Outliers:**\n",
      "   - Identify outliers based on standard deviations from the mean coordinates.\n",
      "   - Correct or remove these values to ensure accurate geospatial representation.\n",
      "\n",
      "2. **Data Type Limitation:**\n",
      "   - Convert latitude and longitude columns to float data types if they are not already stored as such, ensuring that arithmetic operations can be performed correctly.\n",
      "\n",
      "3. **Inter-column Relationship:**\n",
      "   - Ensure consistent use of lat-long pairs across records by verifying each pair is valid (e.g., within geographic boundaries).\n",
      "\n",
      "#### Column_Name: **date-related fields**\n",
      "1. **Missing or Incorrect Date Entries:**\n",
      "   - Identify missing or incorrectly formatted dates and correct them using the median date within a specific time frame to maintain temporal integrity.\n",
      "\n",
      "2. **Consistency across Date Fields:**\n",
      "   - Ensure that all date entries are consistent and properly formatted as datetime objects, preventing errors in any subsequent analyses or operations involving these fields.\n",
      "\n",
      "#### Column_Name: **all numeric fields (like lat, lon)**\n",
      "1. **Potential Data Entry Errors:**\n",
      "   - Implement range checks to identify and correct values outside valid ranges based on domain-specific knowledge, ensuring realistic and accurate data points for analysis and interpretation.\n",
      "\n",
      "### Summary\n",
      "\n",
      "To ensure the overall integrity and accuracy of the dataset:\n",
      "\n",
      "- Address missing or invalid data formats by imputing with appropriate medians, standardizing representation.\n",
      "- Correct geographic mismatches using validated databases to ensure precise regional mapping.\n",
      "- Reduce high cardinality in fields like zipcodes by grouping into more general regions for enhanced analysis.\n",
      "- Remove outliers based on deviations from expected ranges to maintain consistency and reliability.\n",
      "- Ensure consistent formatting of date-related entries as datetime objects to prevent temporal inconsistencies.\n",
      "- Validate numeric values within valid domain-defined ranges to ensure realistic data points.\n",
      "\n",
      "These actions collectively enhance the quality, reliability, and usability of your dataset, facilitating more accurate analytical tasks.\n",
      "[Prompt tokens: 1734, Completion tokens: 991]\n",
      "---------- Validation_Assistant ----------\n",
      "### Validation Report\n",
      "\n",
      "**Overall Status:** Fail\n",
      "\n",
      "### Detailed Findings:\n",
      "\n",
      "#### Column Name: merch_zipcode\n",
      "- **Issue Type:** Missing Values & Invalid Format & Geographic Mismatch & High Cardinality\n",
      "- **Status:** Fail\n",
      "- **Errors/Warnings:** \n",
      "  - The entry should specify only one Recommended Action per issue, but it contains multiple actions for different sub-problems (missing values, invalid format, geographic mismatch, high cardinality).\n",
      "- **Suggestions:**\n",
      "  - Split each separate issue into individual entries with their own Recommended Actions. For example:\n",
      "    - Missing Values\n",
      "      - Impute missing values with the median zipcode value.\n",
      "    - Invalid Format\n",
      "      - Replace non-standard formats (e.g., strings, invalid numbers) with standard integer format.\n",
      "    - Geographic Mismatch\n",
      "      - Cross-check zipcodes against a known geographic dataset to ensure accurate representation.\n",
      "    - High Cardinality\n",
      "      - Group zipcodes into broader regions using a spatial database.\n",
      "\n",
      "#### Column Name: lat, lon\n",
      "- **Issue Type:** Outliers & Data Type Limitation & Inter-column Relationship\n",
      "- **Status:** Fail\n",
      "- **Errors/Warnings:** \n",
      "  - The entry should specify only one Recommended Action per issue, but it contains multiple actions for different sub-problems (outliers, data type conversion, inter-column consistency).\n",
      "- **Suggestions:**\n",
      "  - Split each separate issue into individual entries with their own Recommended Actions. For example:\n",
      "    - Outliers\n",
      "      - Remove coordinates outside a valid range defined by standard deviations from the mean.\n",
      "    - Data Type Limitation\n",
      "      - Ensure consistent float data types for numeric operations and analysis.\n",
      "    - Inter-column Relationship\n",
      "      - Validate lat-long pairs to ensure they are used consistently across records.\n",
      "\n",
      "#### Column Name: date-related fields\n",
      "- **Issue Type:** Missing or Incorrect Date Entries & Consistency across Date Fields\n",
      "- **Status:** Fail\n",
      "- **Errors/Warnings:** \n",
      "  - The entry should specify only one Recommended Action per issue, but it contains multiple actions for different sub-problems (missing/inconsistent dates, uniform formatting).\n",
      "- **Suggestions:**\n",
      "  - Split each separate issue into individual entries with their own Recommended Actions. For example:\n",
      "    - Missing or Incorrect Date Entries\n",
      "      - Correct inconsistencies using the median date within a specific time frame.\n",
      "    - Uniform Formatting\n",
      "      - Ensure all date entries are consistently formatted as datetime objects.\n",
      "\n",
      "#### Column Name: all numeric fields (like lat, lon)\n",
      "- **Issue Type:** Potential Data Entry Errors\n",
      "- **Status:** Fail\n",
      "- **Errors/Warnings:** \n",
      "  - The entry should specify only one Recommended Action per issue, but it contains multiple actions for different sub-problems (range checks).\n",
      "- **Suggestions:**\n",
      "  - Ensure the entry specifies a single action:\n",
      "    - Implement range checks to identify and correct values outside valid ranges based on domain-specific knowledge.\n",
      "\n",
      "These issues must be addressed by splitting each sub-issue into individual entries with distinct Recommended Actions. This will ensure that each problem is clearly defined and can be effectively resolved in isolation.\n",
      "[Prompt tokens: 1936, Completion tokens: 622]\n",
      "---------- Summary ----------\n",
      "Number of messages: 5\n",
      "Finish reason: Maximum number of messages 5 reached, current message count: 5\n",
      "Total prompt tokens: 7766\n",
      "Total completion tokens: 3867\n",
      "Duration: 217.92 seconds\n"
     ]
    }
   ],
   "source": [
    "chat = await cleaning_reasoning_pipeline(initial_profile, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextMessage(source='Validation_Assistant', models_usage=RequestUsage(prompt_tokens=1936, completion_tokens=622), content='### Validation Report\\n\\n**Overall Status:** Fail\\n\\n### Detailed Findings:\\n\\n#### Column Name: merch_zipcode\\n- **Issue Type:** Missing Values & Invalid Format & Geographic Mismatch & High Cardinality\\n- **Status:** Fail\\n- **Errors/Warnings:** \\n  - The entry should specify only one Recommended Action per issue, but it contains multiple actions for different sub-problems (missing values, invalid format, geographic mismatch, high cardinality).\\n- **Suggestions:**\\n  - Split each separate issue into individual entries with their own Recommended Actions. For example:\\n    - Missing Values\\n      - Impute missing values with the median zipcode value.\\n    - Invalid Format\\n      - Replace non-standard formats (e.g., strings, invalid numbers) with standard integer format.\\n    - Geographic Mismatch\\n      - Cross-check zipcodes against a known geographic dataset to ensure accurate representation.\\n    - High Cardinality\\n      - Group zipcodes into broader regions using a spatial database.\\n\\n#### Column Name: lat, lon\\n- **Issue Type:** Outliers & Data Type Limitation & Inter-column Relationship\\n- **Status:** Fail\\n- **Errors/Warnings:** \\n  - The entry should specify only one Recommended Action per issue, but it contains multiple actions for different sub-problems (outliers, data type conversion, inter-column consistency).\\n- **Suggestions:**\\n  - Split each separate issue into individual entries with their own Recommended Actions. For example:\\n    - Outliers\\n      - Remove coordinates outside a valid range defined by standard deviations from the mean.\\n    - Data Type Limitation\\n      - Ensure consistent float data types for numeric operations and analysis.\\n    - Inter-column Relationship\\n      - Validate lat-long pairs to ensure they are used consistently across records.\\n\\n#### Column Name: date-related fields\\n- **Issue Type:** Missing or Incorrect Date Entries & Consistency across Date Fields\\n- **Status:** Fail\\n- **Errors/Warnings:** \\n  - The entry should specify only one Recommended Action per issue, but it contains multiple actions for different sub-problems (missing/inconsistent dates, uniform formatting).\\n- **Suggestions:**\\n  - Split each separate issue into individual entries with their own Recommended Actions. For example:\\n    - Missing or Incorrect Date Entries\\n      - Correct inconsistencies using the median date within a specific time frame.\\n    - Uniform Formatting\\n      - Ensure all date entries are consistently formatted as datetime objects.\\n\\n#### Column Name: all numeric fields (like lat, lon)\\n- **Issue Type:** Potential Data Entry Errors\\n- **Status:** Fail\\n- **Errors/Warnings:** \\n  - The entry should specify only one Recommended Action per issue, but it contains multiple actions for different sub-problems (range checks).\\n- **Suggestions:**\\n  - Ensure the entry specifies a single action:\\n    - Implement range checks to identify and correct values outside valid ranges based on domain-specific knowledge.\\n\\nThese issues must be addressed by splitting each sub-issue into individual entries with distinct Recommended Actions. This will ensure that each problem is clearly defined and can be effectively resolved in isolation.', type='TextMessage')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = await chat.get_messages()\n",
    "chat_history[-1].messages[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
